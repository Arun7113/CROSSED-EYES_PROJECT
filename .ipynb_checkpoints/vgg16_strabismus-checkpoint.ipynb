{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de885aa1-7594-4a96-a242-a6f4fab0ef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting image from strabismus folder\n",
      "'.DS_Store' is not a directory, skipping.\n",
      "Getting image from normal folder\n",
      "num_classes 2\n",
      "Shape of image data (435, 224, 224, 3)\n",
      "number of samples 435\n",
      "target column before encoding ['strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'strabismus', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal', 'normal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               3211392   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17926338 (68.38 MB)\n",
      "Trainable params: 3211650 (12.25 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/55\n",
      "10/10 [==============================] - 5s 485ms/step - loss: 3.1826 - accuracy: 0.5296 - val_loss: 1.3346 - val_accuracy: 0.5714\n",
      "Epoch 2/55\n",
      "10/10 [==============================] - 5s 463ms/step - loss: 3.3788 - accuracy: 0.6250 - val_loss: 0.8362 - val_accuracy: 0.7143\n",
      "Epoch 3/55\n",
      "10/10 [==============================] - 5s 459ms/step - loss: 3.5266 - accuracy: 0.6217 - val_loss: 1.3644 - val_accuracy: 0.7363\n",
      "Epoch 4/55\n",
      "10/10 [==============================] - 5s 467ms/step - loss: 2.4498 - accuracy: 0.7599 - val_loss: 1.4041 - val_accuracy: 0.7253\n",
      "Epoch 5/55\n",
      "10/10 [==============================] - 5s 459ms/step - loss: 2.1103 - accuracy: 0.7697 - val_loss: 0.9338 - val_accuracy: 0.8352\n",
      "Epoch 6/55\n",
      "10/10 [==============================] - 5s 461ms/step - loss: 1.8489 - accuracy: 0.7829 - val_loss: 1.0198 - val_accuracy: 0.8571\n",
      "Epoch 7/55\n",
      "10/10 [==============================] - 5s 460ms/step - loss: 1.6339 - accuracy: 0.8191 - val_loss: 1.0144 - val_accuracy: 0.8022\n",
      "Epoch 8/55\n",
      "10/10 [==============================] - 5s 459ms/step - loss: 1.3887 - accuracy: 0.8257 - val_loss: 1.0740 - val_accuracy: 0.8571\n",
      "Epoch 9/55\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.8848 - accuracy: 0.9079 - val_loss: 0.7835 - val_accuracy: 0.8242\n",
      "Epoch 10/55\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.8571 - accuracy: 0.8849 - val_loss: 1.0223 - val_accuracy: 0.9011\n",
      "Epoch 11/55\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.6226 - accuracy: 0.9178 - val_loss: 0.8197 - val_accuracy: 0.8242\n",
      "Epoch 12/55\n",
      "10/10 [==============================] - 5s 460ms/step - loss: 0.5150 - accuracy: 0.9178 - val_loss: 1.2492 - val_accuracy: 0.8132\n",
      "Epoch 13/55\n",
      "10/10 [==============================] - 5s 472ms/step - loss: 0.3742 - accuracy: 0.9408 - val_loss: 0.7231 - val_accuracy: 0.8571\n",
      "Epoch 14/55\n",
      "10/10 [==============================] - 5s 466ms/step - loss: 0.1890 - accuracy: 0.9638 - val_loss: 0.9160 - val_accuracy: 0.8681\n",
      "Epoch 15/55\n",
      "10/10 [==============================] - 5s 469ms/step - loss: 0.1023 - accuracy: 0.9803 - val_loss: 0.7050 - val_accuracy: 0.8571\n",
      "Epoch 16/55\n",
      "10/10 [==============================] - 5s 466ms/step - loss: 0.1308 - accuracy: 0.9836 - val_loss: 0.7307 - val_accuracy: 0.9231\n",
      "Epoch 17/55\n",
      "10/10 [==============================] - 5s 462ms/step - loss: 0.1295 - accuracy: 0.9671 - val_loss: 0.6346 - val_accuracy: 0.9341\n",
      "Epoch 18/55\n",
      "10/10 [==============================] - 4s 457ms/step - loss: 0.0792 - accuracy: 0.9868 - val_loss: 0.7078 - val_accuracy: 0.8791\n",
      "Epoch 19/55\n",
      "10/10 [==============================] - 4s 457ms/step - loss: 0.0646 - accuracy: 0.9901 - val_loss: 0.8031 - val_accuracy: 0.8791\n",
      "Epoch 20/55\n",
      "10/10 [==============================] - 4s 458ms/step - loss: 0.0585 - accuracy: 0.9868 - val_loss: 0.7489 - val_accuracy: 0.8901\n",
      "Epoch 21/55\n",
      "10/10 [==============================] - 5s 461ms/step - loss: 0.0404 - accuracy: 0.9967 - val_loss: 0.8186 - val_accuracy: 0.8681\n",
      "Epoch 22/55\n",
      "10/10 [==============================] - 5s 467ms/step - loss: 0.0378 - accuracy: 0.9901 - val_loss: 0.8025 - val_accuracy: 0.9121\n",
      "Epoch 23/55\n",
      "10/10 [==============================] - 5s 479ms/step - loss: 0.0248 - accuracy: 0.9967 - val_loss: 0.6760 - val_accuracy: 0.9011\n",
      "Epoch 24/55\n",
      "10/10 [==============================] - 5s 501ms/step - loss: 0.0234 - accuracy: 0.9967 - val_loss: 0.8173 - val_accuracy: 0.9011\n",
      "Epoch 25/55\n",
      "10/10 [==============================] - 5s 525ms/step - loss: 0.0227 - accuracy: 0.9934 - val_loss: 0.8062 - val_accuracy: 0.9121\n",
      "Epoch 26/55\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.0281 - accuracy: 0.9967 - val_loss: 0.6548 - val_accuracy: 0.9121\n",
      "Epoch 27/55\n",
      "10/10 [==============================] - 6s 641ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.5824 - val_accuracy: 0.9341\n",
      "Epoch 28/55\n",
      "10/10 [==============================] - 7s 724ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.9907 - val_accuracy: 0.8352\n",
      "Epoch 29/55\n",
      "10/10 [==============================] - 8s 785ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.8038 - val_accuracy: 0.9011\n",
      "Epoch 30/55\n",
      "10/10 [==============================] - 9s 874ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.7923 - val_accuracy: 0.9011\n",
      "Epoch 31/55\n",
      "10/10 [==============================] - 10s 1s/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.8007 - val_accuracy: 0.8901\n",
      "Epoch 32/55\n",
      "10/10 [==============================] - 10s 976ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6770 - val_accuracy: 0.9231\n",
      "Epoch 33/55\n",
      "10/10 [==============================] - 9s 929ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.8046 - val_accuracy: 0.9341\n",
      "Epoch 34/55\n",
      "10/10 [==============================] - 8s 805ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.7471 - val_accuracy: 0.9121\n",
      "Epoch 35/55\n",
      "10/10 [==============================] - 7s 742ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.9011\n",
      "Epoch 36/55\n",
      "10/10 [==============================] - 7s 712ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.7223 - val_accuracy: 0.9011\n",
      "Epoch 37/55\n",
      "10/10 [==============================] - 7s 698ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.9451\n",
      "Epoch 38/55\n",
      "10/10 [==============================] - 7s 724ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.9341\n",
      "Epoch 39/55\n",
      "10/10 [==============================] - 7s 726ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6770 - val_accuracy: 0.9451\n",
      "Epoch 40/55\n",
      "10/10 [==============================] - 8s 803ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.9121\n",
      "Epoch 41/55\n",
      "10/10 [==============================] - 7s 699ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.9451\n",
      "Epoch 42/55\n",
      "10/10 [==============================] - 7s 694ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8420 - val_accuracy: 0.8791\n",
      "Epoch 43/55\n",
      "10/10 [==============================] - 7s 715ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6740 - val_accuracy: 0.9011\n",
      "Epoch 44/55\n",
      "10/10 [==============================] - 7s 719ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 0.9560\n",
      "Epoch 45/55\n",
      "10/10 [==============================] - 7s 708ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.9121\n",
      "Epoch 46/55\n",
      "10/10 [==============================] - 7s 717ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 0.9011\n",
      "Epoch 47/55\n",
      "10/10 [==============================] - 8s 784ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.9121\n",
      "Epoch 48/55\n",
      "10/10 [==============================] - 7s 759ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5933 - val_accuracy: 0.9560\n",
      "Epoch 49/55\n",
      "10/10 [==============================] - 8s 781ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5764 - val_accuracy: 0.9341\n",
      "Epoch 50/55\n",
      "10/10 [==============================] - 8s 817ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6886 - val_accuracy: 0.9011\n",
      "Epoch 51/55\n",
      "10/10 [==============================] - 8s 868ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 0.9011\n",
      "Epoch 52/55\n",
      "10/10 [==============================] - 9s 879ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.9231\n",
      "Epoch 53/55\n",
      "10/10 [==============================] - 9s 920ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7667 - val_accuracy: 0.9011\n",
      "Epoch 54/55\n",
      "10/10 [==============================] - 9s 903ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.9121\n",
      "Epoch 55/55\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.9560\n",
      "3/3 [==============================] - 2s 617ms/step - loss: 0.6047 - accuracy: 0.9560\n",
      "Test Loss: 0.6046756505966187\n",
      "Test Accuracy: 0.9560439586639404\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import argmax\n",
    "\n",
    "PATH = \"augmented_image\"\n",
    "data_dir_list = os.listdir(PATH)\n",
    "data_dir_list\n",
    "\n",
    "img_rows=224\n",
    "img_cols=224\n",
    "num_channel=3\n",
    "\n",
    "num_epoch = 55\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "img_data_list = []\n",
    "classes_names_list = []\n",
    "target_column = []\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    if os.path.isdir(os.path.join(PATH, dataset)):  \n",
    "        classes_names_list.append(dataset)\n",
    "        print(\"Getting image from {} folder\".format(dataset))\n",
    "        img_list = os.listdir(os.path.join(PATH, dataset))\n",
    "        for img in img_list:\n",
    "            input_img = cv2.imread(os.path.join(PATH, dataset, img))\n",
    "            input_img_resize = cv2.resize(input_img, (img_rows, img_cols))\n",
    "            img_data_list.append(input_img_resize)\n",
    "            target_column.append(dataset)\n",
    "    else:\n",
    "        print(\"'{}' is not a directory, skipping.\".format(dataset))\n",
    "\n",
    "num_classes = len(classes_names_list)\n",
    "print(\"num_classes\",num_classes)\n",
    "img_data = np.array(img_data_list) \n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print(\"Shape of image data\",img_data.shape)\n",
    "num_of_samples = img_data.shape[0]\n",
    "input_shape = img_data[0].shape \n",
    "print(\"number of samples\",num_of_samples)\n",
    "print(\"target column before encoding\",target_column)\n",
    "\n",
    "Labelencoder = LabelEncoder()\n",
    "target_column = Labelencoder.fit_transform(target_column)\n",
    "np.unique(target_column)\n",
    "\n",
    "target_column\n",
    "\n",
    "target_column_hotcoded = to_categorical(target_column,num_classes)\n",
    "X,Y = shuffle(img_data,target_column_hotcoded,random_state=2)\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(X,Y,test_size=0.3,random_state=2)\n",
    "X_test,X_val,y_test,y_val = train_test_split(X_temp,y_temp,test_size=0.3,random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, num_channel))\n",
    "\n",
    "\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(vgg_model)\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test Accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
